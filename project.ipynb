{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn, torch\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_csv(data_file,train_test = 'train'):\n",
    "    if train_test == 'train':\n",
    "        with open(data_file, 'r', encoding = 'utf-8') as fd:\n",
    "            data = [l.strip().split('\\t') for l in fd.readlines()][1:]\n",
    "        X = [d[2] for d in data]\n",
    "        y = [d[1] for d in data]\n",
    "        return X, y\n",
    "    elif train_test == \"test\":\n",
    "        with open(data_file, 'r') as fd:\n",
    "            data = [l.strip().split('\\t') for l in fd.readlines()][1:]\n",
    "        X = [d[1] for d in data]\n",
    "        return X        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tweet, train_label = parse_csv('data/train/SemEval2018-T3-train-taskB_emoji_ironyHashtags.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tweet, test_label = parse_csv('data/gold/SemEval2018-T3_gold_test_taskB_emoji.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {\n",
    "    \"train\": (train_tweet, train_label),\n",
    "    \"gold\": (test_tweet, test_label),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ekphrasis.classes.preprocessor import TextPreProcessor\n",
    "from ekphrasis.classes.tokenizer import SocialTokenizer\n",
    "from ekphrasis.dicts.emoticons import emoticons\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def twitter_preprocess():\n",
    "    preprocessor = TextPreProcessor(\n",
    "        normalize=['url', 'email', 'percent', 'money', 'phone', 'user',\n",
    "                   'time',\n",
    "                   'date', 'number'],\n",
    "        annotate={\"hashtag\", \"elongated\", \"allcaps\", \"repeated\", 'emphasis',\n",
    "                  'censored'},\n",
    "        all_caps_tag=\"wrap\",\n",
    "        fix_text=True,\n",
    "        segmenter=\"twitter_2018\",\n",
    "        corrector=\"twitter_2018\",\n",
    "        unpack_hashtags=True,\n",
    "        unpack_contractions=True,\n",
    "        spell_correct_elong=False,\n",
    "        tokenizer=SocialTokenizer(lowercase=True).tokenize,\n",
    "        dicts=[emoticons]\n",
    "    ).pre_process_doc\n",
    "\n",
    "    def preprocess(name, dataset):\n",
    "        desc = \"PreProcessing dataset {}...\".format(name)\n",
    "\n",
    "        data = [preprocessor(x)\n",
    "                for x in tqdm(dataset, desc=desc)]\n",
    "        return data\n",
    "\n",
    "    return preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading twitter_2018 - 1grams ...\n",
      "Reading twitter_2018 - 2grams ...\n",
      "Reading twitter_2018 - 1grams ...\n"
     ]
    }
   ],
   "source": [
    "pre = twitter_preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PreProcessing dataset None...: 100%|██████████████████████████████████████████████| 3834/3834 [00:05<00:00, 658.02it/s]\n"
     ]
    }
   ],
   "source": [
    "a = pre(None,train_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH = \"C:/Users/ABC/Desktop/630/project/ntua_slp/ntua-slp-semeval2018\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:/Users/ABC/Desktop/630/project/ntua_slp/ntua-slp-semeval2018\\\\embeddings\\\\ntua_twitter_affect_310.txt'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_conf = {\n",
    "    \"name\": \"TASK3_B\",\n",
    "    \"token_type\": \"word\",\n",
    "    \"batch_train\": 32,\n",
    "    \"batch_eval\": 32,\n",
    "   \"epochs\": 50,\n",
    "    \"embeddings_file\": \"ntua_twitter_affect_310\",\n",
    "    \"embed_dim\": 310,\n",
    "    \"embed_finetune\": False,\n",
    "    \"embed_noise\": 0.2,\n",
    "    \"embed_dropout\": 0.1,\n",
    "    \"encoder_dropout\": 0.2,\n",
    "    \"encoder_size\": 150,\n",
    "    \"encoder_layers\": 2,\n",
    "    \"encoder_bidirectional\": True,\n",
    "    \"attention\": True,\n",
    "    \"attention_layers\": 1,\n",
    "    \"attention_context\": False,\n",
    "    \"attention_activation\": \"tanh\",\n",
    "    \"attention_dropout\": 0.0,\n",
    "    \"base\": 0.3,\n",
    "    \"patience\": 10,\n",
    "    \"weight_decay\": 0.0,\n",
    "    \"clip_norm\": 1,\n",
    "}\n",
    "os.path.join(BASE_PATH, \"embeddings\",\n",
    "                                \"{}.txt\".format(model_conf[\"embeddings_file\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import errno\n",
    "import pickle\n",
    "\n",
    "def file_cache_name(file):\n",
    "    head, tail = os.path.split(file)\n",
    "    filename, ext = os.path.splitext(tail)\n",
    "    return os.path.join(head, filename + \".p\")\n",
    "\n",
    "\n",
    "def write_cache_word_vectors(file, data):\n",
    "    with open(file_cache_name(file), 'wb') as pickle_file:\n",
    "        pickle.dump(data, pickle_file)\n",
    "\n",
    "\n",
    "def load_cache_word_vectors(file):\n",
    "    with open(file_cache_name(file), 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "\n",
    "def load_word_vectors(file, dim):\n",
    "    \"\"\"\n",
    "    Read the word vectors from a text file\n",
    "    Args:\n",
    "        file (): the filename\n",
    "        dim (): the dimensions of the word vectors\n",
    "\n",
    "    Returns:\n",
    "        word2idx (dict): dictionary of words to ids\n",
    "        idx2word (dict): dictionary of ids to words\n",
    "        embeddings (np.ndarray): the word embeddings matrix\n",
    "\n",
    "    \"\"\"\n",
    "    # in order to avoid this time consuming operation, cache the results\n",
    "    try:\n",
    "        cache = load_cache_word_vectors(file)\n",
    "        print(\"Loaded word embeddings from cache.\")\n",
    "        return cache\n",
    "    except OSError:\n",
    "        print(\"Didn't find embeddings cache file {}\".format(file))\n",
    "\n",
    "    # create the necessary dictionaries and the word embeddings matrix\n",
    "    if os.path.exists(file):\n",
    "        print('Indexing file {} ...'.format(file))\n",
    "\n",
    "        word2idx = {}  # dictionary of words to ids\n",
    "        idx2word = {}  # dictionary of ids to words\n",
    "        embeddings = []  # the word embeddings matrix\n",
    "\n",
    "        # create the 2D array, which will be used for initializing\n",
    "        # the Embedding layer of a NN.\n",
    "        # We reserve the first row (idx=0), as the word embedding,\n",
    "        # which will be used for zero padding (word with id = 0).\n",
    "        embeddings.append(np.zeros(dim))\n",
    "\n",
    "        # flag indicating whether the first row of the embeddings file\n",
    "        # has a header\n",
    "        header = False\n",
    "\n",
    "        # read file, line by line\n",
    "        with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "            for i, line in enumerate(f, 1):\n",
    "\n",
    "                # skip the first row if it is a header\n",
    "                if i == 1:\n",
    "                    if len(line.split()) < dim:\n",
    "                        header = True\n",
    "                        continue\n",
    "\n",
    "                values = line.split(\" \")\n",
    "                word = values[0]\n",
    "                vector = np.asarray(values[1:], dtype='float32')\n",
    "\n",
    "                index = i - 1 if header else i\n",
    "\n",
    "                idx2word[index] = word\n",
    "                word2idx[word] = index\n",
    "                embeddings.append(vector)\n",
    "\n",
    "            # add an unk token, for OOV words\n",
    "            if \"<unk>\" not in word2idx:\n",
    "                idx2word[len(idx2word) + 1] = \"<unk>\"\n",
    "                word2idx[\"<unk>\"] = len(word2idx) + 1\n",
    "                embeddings.append(\n",
    "                    np.random.uniform(low=-0.05, high=0.05, size=dim))\n",
    "\n",
    "            print(set([len(x) for x in embeddings]))\n",
    "\n",
    "            print('Found %s word vectors.' % len(embeddings))\n",
    "            embeddings = np.array(embeddings, dtype='float32')\n",
    "\n",
    "        # write the data to a cache file\n",
    "        write_cache_word_vectors(file, (word2idx, idx2word, embeddings))\n",
    "\n",
    "        return word2idx, idx2word, embeddings\n",
    "\n",
    "    else:\n",
    "        print(\"{} not found!\".format(file))\n",
    "        raise OSError(errno.ENOENT, os.strerror(errno.ENOENT), file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK3_A = {\n",
    "#     \"name\": \"TASK3_A\",\n",
    "#     \"token_type\": \"word\",\n",
    "#     \"batch_train\": 64,\n",
    "#     \"batch_eval\": 64,\n",
    "#    \"epochs\": 50,\n",
    "#     \"embeddings_file\": \"ntua_twitter_affect_310\",\n",
    "#     \"embed_dim\": 310,\n",
    "#     \"embed_finetune\": False,\n",
    "#     \"embed_noise\": 0.05,\n",
    "#     \"embed_dropout\": 0.1,\n",
    "#     \"encoder_dropout\": 0.2,\n",
    "#     \"encoder_size\": 150,\n",
    "#     \"encoder_layers\": 2,\n",
    "#     \"encoder_bidirectional\": True,\n",
    "#     \"attention\": True,\n",
    "#     \"attention_layers\": 1,\n",
    "#     \"attention_context\": False,\n",
    "#     \"attention_activation\": \"tanh\",\n",
    "#     \"attention_dropout\": 0.0,\n",
    "#     \"base\": 0.7,\n",
    "#     \"patience\": 10,\n",
    "#     \"weight_decay\": 0.0,\n",
    "#     \"clip_norm\": 1,\n",
    "# }\n",
    "TASK3_B = {\n",
    "    \"name\": \"TASK3_B\",\n",
    "    \"token_type\": \"word\",\n",
    "    \"batch_train\": 32,\n",
    "    \"batch_eval\": 32,\n",
    "   \"epochs\": 50,\n",
    "    \"embeddings_file\": \"ntua_twitter_affect_310\",\n",
    "    \"embed_dim\": 310,\n",
    "    \"embed_finetune\": False,\n",
    "    \"embed_noise\": 0.2,\n",
    "    \"embed_dropout\": 0.1,\n",
    "    \"encoder_dropout\": 0.2,\n",
    "    \"encoder_size\": 150,\n",
    "    \"encoder_layers\": 2,\n",
    "    \"encoder_bidirectional\": True,\n",
    "    \"attention\": True,\n",
    "    \"attention_layers\": 1,\n",
    "    \"attention_context\": False,\n",
    "    \"attention_activation\": \"tanh\",\n",
    "    \"attention_dropout\": 0.0,\n",
    "    \"base\": 0.3,\n",
    "    \"patience\": 10,\n",
    "    \"weight_decay\": 0.0,\n",
    "    \"clip_norm\": 1,\n",
    "}\n",
    "def load_embeddings(model_conf):\n",
    "    word_vectors = os.path.join(BASE_PATH, \"embeddings\",\n",
    "                                \"{}.txt\".format(model_conf[\"embeddings_file\"]))\n",
    "    word_vectors_size = model_conf[\"embed_dim\"]\n",
    "\n",
    "    # load word embeddings\n",
    "    print(\"loading word embeddings...\")\n",
    "    return load_word_vectors(word_vectors, word_vectors_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading word embeddings...\n",
      "Loaded word embeddings from cache.\n"
     ]
    }
   ],
   "source": [
    "word2idx, idx2word, embeddings = load_embeddings(TASK3_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from collections import Counter\n",
    "\n",
    "def vectorize(sequence, el2idx, max_length):\n",
    "    \"\"\"\n",
    "    Covert array of tokens, to array of ids, with a fixed length\n",
    "    and zero padding at the end\n",
    "    Args:\n",
    "        sequence (): a list of elements\n",
    "        el2idx (): dictionary of word to ids\n",
    "        max_length ():\n",
    "        unk_policy (): how to handle OOV words\n",
    "        spell_corrector (): if unk_policy = 'correct' then pass a callable\n",
    "            which will try to apply spell correction to the OOV token\n",
    "\n",
    "\n",
    "    Returns: list of ids with zero padding at the end\n",
    "\n",
    "    \"\"\"\n",
    "    words = np.zeros(max_length).astype(int)\n",
    "\n",
    "    # trim tokens after max length\n",
    "    sequence = sequence[:max_length]\n",
    "\n",
    "    for i, token in enumerate(sequence):\n",
    "        if token in el2idx:\n",
    "            words[i] = el2idx[token]\n",
    "        else:\n",
    "            words[i] = el2idx[\"<unk>\"]\n",
    "\n",
    "    return words\n",
    "\n",
    "class WordDataset(Dataset):\n",
    "\n",
    "    def __init__(self, X, y, word2idx, pre):\n",
    "        \"\"\"\n",
    "        A PyTorch Dataset\n",
    "        What we have to do is to implement the 2 abstract methods:\n",
    "\n",
    "            - __len__(self): in order to let the DataLoader know the size\n",
    "                of our dataset and to perform batching, shuffling and so on...\n",
    "            - __getitem__(self, index): we have to return the properly\n",
    "                processed data-item from our dataset with a given index\n",
    "\n",
    "        Args:\n",
    "            X (): list of training samples\n",
    "            y (): list of training labels\n",
    "            max_length (int): the max length for each sentence.\n",
    "                if 0 then use the maximum length in the dataset\n",
    "            word2idx (dict): a dictionary which maps words to indexes\n",
    "            label_transformer (LabelTransformer):\n",
    "        \"\"\"\n",
    "        self.word2idx = word2idx\n",
    "        \n",
    "        self.data = X\n",
    "        self.labels = y\n",
    "        \n",
    "        self.data = pre(None, self.data)\n",
    "        \n",
    "        self.set_max_length()\n",
    "\n",
    "        self.dataset_statistics()\n",
    "\n",
    "    def set_max_length(self):\n",
    "        self.max_length = max([len(x) for x in self.data])\n",
    "\n",
    "    def dataset_statistics(self):\n",
    "        words = Counter()\n",
    "        for x in self.data:\n",
    "            words.update(x)\n",
    "        unks = {w: v for w, v in words.items() if w not in self.word2idx}\n",
    "        # unks = sorted(unks.items(), key=lambda x: x[1], reverse=True)\n",
    "        total_words = sum(words.values())\n",
    "        total_unks = sum(unks.values())\n",
    "\n",
    "        print(\"Total words: {}, Total unks:{} ({:.2f}%)\".format(\n",
    "            total_words, total_unks, total_unks * 100 / total_words))\n",
    "\n",
    "        print(\"Unique words: {}, Unique unks:{} ({:.2f}%)\".format(\n",
    "            len(words), len(unks), len(unks) * 100 / len(words)))\n",
    "\n",
    "        # label statistics\n",
    "        print(\"Labels statistics:\")\n",
    "        if isinstance(self.labels[0], float):\n",
    "            print(\"Mean:{:.4f}, Std:{:.4f}\".format(np.mean(self.labels),\n",
    "                                                   np.std(self.labels)))\n",
    "        else:\n",
    "            try:\n",
    "                counts = Counter(self.labels)\n",
    "                stats = {k: \"{:.2f}%\".format(v * 100 / len(self.labels))\n",
    "                         for k, v in sorted(counts.items())}\n",
    "                print(stats)\n",
    "            except:\n",
    "                print(\"Not implemented for mclf\")\n",
    "        print()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Returns the _transformed_ item from the dataset\n",
    "\n",
    "        Args:\n",
    "            index (int):\n",
    "\n",
    "        Returns:\n",
    "            (tuple):\n",
    "                * example (ndarray): vector representation of a training sample\n",
    "                * label (string): the class label\n",
    "                * length (int): the length (tokens) of the sentence\n",
    "                * index (int): the index of the dataitem in the dataset.\n",
    "                  It is useful for getting the raw input for visualizations.\n",
    "        \"\"\"\n",
    "        sample, label = self.data[index], self.labels[index]\n",
    "\n",
    "        # transform the sample and the label,\n",
    "        # in order to feed them to the model\n",
    "        sample = vectorize(sample, self.word2idx, self.max_length)\n",
    "\n",
    "        if isinstance(label, (list, tuple)):\n",
    "            label = np.array(label)\n",
    "\n",
    "        return sample, label, len(self.data[index]), index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building word-level datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PreProcessing dataset None...: 100%|█████████████████████████████████████████████| 3834/3834 [00:02<00:00, 1335.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words: 81386, Total unks:199 (0.24%)\n",
      "Unique words: 9111, Unique unks:197 (2.16%)\n",
      "Labels statistics:\n",
      "{'0': '50.16%', '1': '36.25%', '2': '8.24%', '3': '5.35%'}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PreProcessing dataset None...: 100%|████████████████████████████████████████████████| 784/784 [00:01<00:00, 728.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words: 17462, Total unks:42 (0.24%)\n",
      "Unique words: 3420, Unique unks:42 (1.23%)\n",
      "Labels statistics:\n",
      "{'0': '60.33%', '1': '20.92%', '2': '10.84%', '3': '7.91%'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "loaders={}\n",
    "from torch.utils.data import DataLoader\n",
    "print(\"Building word-level datasets...\")\n",
    "for k, v in datasets.items():\n",
    "    dataset = WordDataset(v[0], v[1], word2idx, pre)\n",
    "    batch_size = TASK3_B[\"batch_train\"] if k == \"train\" else TASK3_B[\"batch_eval\"]\n",
    "    loaders[k] = DataLoader(dataset, batch_size, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelHelper:\n",
    "    @staticmethod\n",
    "    def _sort_by(lengths):\n",
    "        \"\"\"\n",
    "        Sort batch data and labels by length.\n",
    "        Useful for variable length inputs, for utilizing PackedSequences\n",
    "        Args:\n",
    "            lengths (nn.Tensor): tensor containing the lengths for the data\n",
    "\n",
    "        Returns:\n",
    "            - sorted lengths Tensor\n",
    "            - sort (callable) which will sort a given iterable\n",
    "                according to lengths\n",
    "            - unsort (callable) which will revert a given iterable to its\n",
    "                original order\n",
    "\n",
    "        \"\"\"\n",
    "        batch_size = lengths.size(0)\n",
    "\n",
    "        sorted_lengths, sorted_idx = lengths.sort()\n",
    "        _, original_idx = sorted_idx.sort(0, descending=True)\n",
    "        reverse_idx = torch.linspace(batch_size - 1, 0, batch_size).long()\n",
    "\n",
    "        reverse_idx = reverse_idx.to(device)\n",
    "\n",
    "        sorted_lengths = sorted_lengths[reverse_idx]\n",
    "\n",
    "        def sort(iterable):\n",
    "            if len(iterable.shape) > 1:\n",
    "                return iterable[sorted_idx.data][reverse_idx]\n",
    "            else:\n",
    "                return iterable\n",
    "\n",
    "        def unsort(iterable):\n",
    "            if len(iterable.shape) > 1:\n",
    "                return iterable[reverse_idx][original_idx][reverse_idx]\n",
    "            else:\n",
    "                return iterable\n",
    "\n",
    "        return sorted_lengths, sort, unsort\n",
    "\n",
    "class RNN(nn.Module, ModelHelper):\n",
    "    def __init__(self, input_size, rnn_size, num_layers,\n",
    "                 bidirectional, dropout, embd, task = 'B'):\n",
    "        \"\"\"\n",
    "        A simple RNN Encoder.\n",
    "\n",
    "        Args:\n",
    "            input_size (int): the size of the input features\n",
    "            rnn_size (int):\n",
    "            num_layers (int):\n",
    "            bidirectional (bool):\n",
    "            dropout (float):\n",
    "\n",
    "        Returns: outputs, last_outputs\n",
    "        - **outputs** of shape `(batch, seq_len, hidden_size)`:\n",
    "          tensor containing the output features `(h_t)`\n",
    "          from the last layer of the LSTM, for each t.\n",
    "        - **last_outputs** of shape `(batch, hidden_size)`:\n",
    "          tensor containing the last output features\n",
    "          from the last layer of the LSTM, for each t=seq_len.\n",
    "\n",
    "        \"\"\"\n",
    "        super(RNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings=embd.shape[0],embedding_dim=embd.shape[1])\n",
    "        \n",
    "        self.init_embeddings(embd)\n",
    "\n",
    "        self.rnn = nn.LSTM(input_size=input_size,\n",
    "                           hidden_size=rnn_size,\n",
    "                           num_layers=num_layers,\n",
    "                           bidirectional=bidirectional,\n",
    "                           dropout=dropout,\n",
    "                           batch_first=True)\n",
    "\n",
    "        # the dropout \"layer\" for the output of the RNN\n",
    "        self.drop_rnn = nn.Dropout(dropout)\n",
    "\n",
    "        # define output feature size\n",
    "        self.feature_size = rnn_size\n",
    "\n",
    "        if bidirectional:\n",
    "            self.feature_size *= 2\n",
    "            \n",
    "        if task == 'B':\n",
    "            self.linear = nn.Linear(self.feature_size, 4)\n",
    "        else:\n",
    "            self.linear = nn.Linear(self.feature_size, 1)\n",
    "            \n",
    "    def init_embeddings(self, weights):\n",
    "        self.embedding.weight = nn.Parameter(torch.from_numpy(weights),requires_grad=False)\n",
    "\n",
    "    @staticmethod\n",
    "    def last_by_index(outputs, lengths):\n",
    "        # Index of the last output for each sequence.\n",
    "        idx = (lengths - 1).view(-1, 1).expand(outputs.size(0),\n",
    "                                               outputs.size(2)).unsqueeze(1)\n",
    "        return outputs.gather(1, idx).squeeze()\n",
    "\n",
    "    @staticmethod\n",
    "    def split_directions(outputs):\n",
    "        direction_size = int(outputs.size(-1) / 2)\n",
    "        forward = outputs[:, :, :direction_size]\n",
    "        backward = outputs[:, :, direction_size:]\n",
    "        return forward, backward\n",
    "\n",
    "    def last_timestep(self, outputs, lengths, bi=False):\n",
    "        if bi:\n",
    "            forward, backward = self.split_directions(outputs)\n",
    "            last_forward = self.last_by_index(forward, lengths)\n",
    "            last_backward = backward[:, 0, :]\n",
    "            return torch.cat((last_forward, last_backward), dim=-1)\n",
    "\n",
    "        else:\n",
    "            return self.last_by_index(outputs, lengths)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        \"\"\"\n",
    "        This is the heart of the model. This function, defines how the data\n",
    "        passes through the network.\n",
    "        Args:\n",
    "            embs (): word embeddings\n",
    "            lengths (): the lengths of each sentence\n",
    "\n",
    "        Returns: the logits for each class\n",
    "\n",
    "        \"\"\"\n",
    "        lengths, sort, unsort = self._sort_by(lengths)\n",
    "        x = sort(x)\n",
    "\n",
    "        embd = self.embedding(x.long())\n",
    "        \n",
    "        # pack the batch\n",
    "        packed = pack_padded_sequence(embd, list(lengths.data),\n",
    "                                      batch_first=True)\n",
    "\n",
    "        out_packed, _ = self.rnn(packed)\n",
    "\n",
    "        # unpack output - no need if we are going to use only the last outputs\n",
    "        outputs, _ = pad_packed_sequence(out_packed, batch_first=True)\n",
    "\n",
    "        # get the outputs from the last *non-masked* timestep for each sentence\n",
    "        last_outputs = self.last_timestep(outputs, lengths,\n",
    "                                          self.rnn.bidirectional)\n",
    "\n",
    "        # apply dropout to the outputs of the RNN\n",
    "        last_outputs = self.drop_rnn(last_outputs)\n",
    "\n",
    "        # unsort\n",
    "        last_outputs = unsort(last_outputs)\n",
    "\n",
    "        logits = self.linear(last_outputs)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_training(model, loaders, epochs=10, lr=0.001, clip=5, print_every=100):\n",
    "    model.train()\n",
    "    \n",
    "    opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    loss_f = nn.MultiLabelSoftMarginLoss()\n",
    "    \n",
    "    gpu = torch.cuda.is_available()\n",
    "    \n",
    "    if(gpu):\n",
    "        device = torch.device('cuda')\n",
    "        model.to(device)\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "    \n",
    "    for e in range(epochs):\n",
    "        counter = 0\n",
    "        for i_batch, (X, y, lengths, index) in enumerate(loaders['train'], 1):\n",
    "            counter += 1\n",
    "            \n",
    "            y = np.array(list(map(int,y)))\n",
    "            y_onehot = np.zeros((y.shape[0], 4))\n",
    "            y_onehot[np.arange(y.shape[0]), y] = 1\n",
    "            y = torch.from_numpy(y_onehot).float()\n",
    "            if(gpu):\n",
    "                X, y, lengths = X.to(device), y.to(device), lengths.to(device)\n",
    "\n",
    "            opt.zero_grad()\n",
    "            \n",
    "            linear_outputs = model(X, lengths)\n",
    "            \n",
    "            loss = loss_f(linear_outputs, y)\n",
    "            loss.backward()\n",
    "            \n",
    "            nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "            opt.step()\n",
    "        \n",
    "            if counter % print_every == 0:\n",
    "                print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                      \"Step: {}...\".format(counter),\n",
    "                      \"Loss: {:.4f}...\".format(loss.item()))\n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNN(TASK3_B['embed_dim'], TASK3_B['encoder_size'], TASK3_B['encoder_layers'], TASK3_B['encoder_bidirectional'], TASK3_B['encoder_dropout'], embeddings, task = 'B')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model\n",
    "gpu = torch.cuda.is_available()\n",
    "    \n",
    "if(gpu):\n",
    "    device = torch.device('cuda')\n",
    "    model.to(device)\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10... Step: 100... Loss: 0.3814...\n",
      "Epoch: 2/10... Step: 100... Loss: 0.2152...\n",
      "Epoch: 3/10... Step: 100... Loss: 0.2992...\n",
      "Epoch: 4/10... Step: 100... Loss: 0.1606...\n",
      "Epoch: 5/10... Step: 100... Loss: 0.2329...\n",
      "Epoch: 6/10... Step: 100... Loss: 0.1676...\n",
      "Epoch: 7/10... Step: 100... Loss: 0.1334...\n",
      "Epoch: 8/10... Step: 100... Loss: 0.1314...\n",
      "Epoch: 9/10... Step: 100... Loss: 0.2312...\n",
      "Epoch: 10/10... Step: 100... Loss: 0.1417...\n"
     ]
    }
   ],
   "source": [
    "model_training(model, loaders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:251: UserWarning: Couldn't retrieve source code for container of type RNN. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "torch.save(model, 'model_b_310_10e.mdl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PreProcessing dataset None...: 100%|███████████████████████████████████████████████| 784/784 [00:00<00:00, 1187.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words: 17462, Total unks:42 (0.24%)\n",
      "Unique words: 3420, Unique unks:42 (1.23%)\n",
      "Labels statistics:\n",
      "{'0': '60.33%', '1': '20.92%', '2': '10.84%', '3': '7.91%'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gold_data = WordDataset(datasets['gold'][0],datasets['gold'][1],word2idx,pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_size = len(gold_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xs = []\n",
    "ys = []\n",
    "lengths = []\n",
    "for X, y, length, index in gold_data:\n",
    "    Xs.append(torch.from_numpy(X))\n",
    "    ys.append(int(y))\n",
    "    lengths.append(int(length))\n",
    "Xs = torch.stack(Xs)\n",
    "ys = torch.tensor(ys)\n",
    "lengths = torch.tensor(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNN(\n",
       "  (embedding): Embedding(804871, 310)\n",
       "  (rnn): LSTM(310, 150, num_layers=2, batch_first=True, dropout=0.2, bidirectional=True)\n",
       "  (drop_rnn): Dropout(p=0.2)\n",
       "  (linear): Linear(in_features=300, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_outputs = model(Xs.to(torch.device('cuda')), lengths.to(torch.device('cuda')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_argmax = test_outputs.max(1)[1].to(torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8609693646430969"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision = torch.mean((test_argmax == ys).float()).item()\n",
    "precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "argmax_list = [tensor.item() for tensor in test_outputs.max(1)[1]]\n",
    "ys_list = [tensor.item() for tensor in ys]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = f1_score(ys_list, argmax_list, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7023853181274253"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "0.84375\n",
      "<class 'torch.Tensor'>\n",
      "0.8125\n",
      "<class 'torch.Tensor'>\n",
      "0.875\n",
      "<class 'torch.Tensor'>\n",
      "0.90625\n",
      "<class 'torch.Tensor'>\n",
      "0.9375\n",
      "<class 'torch.Tensor'>\n",
      "0.78125\n",
      "<class 'torch.Tensor'>\n",
      "0.9375\n",
      "<class 'torch.Tensor'>\n",
      "0.84375\n",
      "<class 'torch.Tensor'>\n",
      "0.8125\n",
      "<class 'torch.Tensor'>\n",
      "0.875\n",
      "<class 'torch.Tensor'>\n",
      "0.84375\n",
      "<class 'torch.Tensor'>\n",
      "0.78125\n",
      "<class 'torch.Tensor'>\n",
      "0.90625\n",
      "<class 'torch.Tensor'>\n",
      "0.875\n",
      "<class 'torch.Tensor'>\n",
      "0.875\n",
      "<class 'torch.Tensor'>\n",
      "0.78125\n",
      "<class 'torch.Tensor'>\n",
      "0.9375\n",
      "<class 'torch.Tensor'>\n",
      "0.78125\n",
      "<class 'torch.Tensor'>\n",
      "0.8125\n",
      "<class 'torch.Tensor'>\n",
      "0.90625\n",
      "<class 'torch.Tensor'>\n",
      "0.84375\n",
      "<class 'torch.Tensor'>\n",
      "0.78125\n",
      "<class 'torch.Tensor'>\n",
      "0.9375\n",
      "<class 'torch.Tensor'>\n",
      "0.78125\n"
     ]
    }
   ],
   "source": [
    "loss_f = torch.nn.MultiLabelSoftMarginLoss()\n",
    "X_list = []\n",
    "y_list = []\n",
    "for i_batch, (X, y, lengths, index) in enumerate(loaders['gold'], 1):\n",
    "    y_raw = np.array(list(map(int,y)))\n",
    "    y_onehot = np.zeros((y_raw.shape[0], 4))\n",
    "    y_onehot[np.arange(y_raw.shape[0]), y_raw] = 1\n",
    "    y = torch.from_numpy(y_onehot).float()\n",
    "    if(gpu):\n",
    "        X, y, lengths = X.to(device), y.to(device), lengths.to(device)\n",
    "            \n",
    "    linear_outputs = model(X, lengths) \n",
    "    loss = loss_f(linear_outputs, y)\n",
    "    argmax = linear_outputs.max(1)[1]\n",
    "    print(np.mean(np.array(argmax.to(torch.device('cpu'))) == y_raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
